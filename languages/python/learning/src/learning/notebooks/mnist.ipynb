{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9300fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download mnist dataset\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "mnist = datasets.MNIST(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab4b425",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "835c7b4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_device():\n",
    "    if torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    if torch.cuda.is_available(): \n",
    "        return torch.device(\"cuda\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "device = get_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c41f0c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and validation sets\n",
    "# Use 80% for training and 20% for validation\n",
    "from torch.utils.data import random_split\n",
    "train_size = int(0.8 * len(mnist))\n",
    "val_size = int(0.1*len(mnist))\n",
    "test_size = len(mnist) - train_size - val_size\n",
    "mnist_train, mnist_val, mnist_test = random_split(mnist, [train_size, val_size, test_size])\n",
    "\n",
    "train_data = mnist.data[mnist_train.indices]/255.0 \n",
    "train_targets = mnist.targets[mnist_train.indices]\n",
    "\n",
    "val_data = mnist.data[mnist_val.indices]/255.0\n",
    "val_targets = mnist.targets[mnist_val.indices]\n",
    "\n",
    "test_data = mnist.data[mnist_test.indices]/255.0\n",
    "test_targets = mnist.targets[mnist_test.indices]\n",
    "\n",
    "val_data, val_targets = val_data.to(device), val_targets.to(device)\n",
    "test_data, test_targets = test_data.to(device), test_targets.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98530060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 48000\n",
      "Shape of each sample: torch.Size([28, 28])\n",
      "Number of validation samples: 6000\n",
      "Data type of each sample: torch.uint8\n",
      "Data type of labels: torch.int64\n"
     ]
    }
   ],
   "source": [
    "# Show the size and shape and datatype of the dataset\n",
    "print(\"Number of training samples:\", len(mnist_train))\n",
    "print(\"Shape of each sample:\", mnist.data[0].shape)\n",
    "print(\"Number of validation samples:\", len(mnist_val))\n",
    "print(\"Data type of each sample:\", mnist.data[0].dtype)\n",
    "print(\"Data type of labels:\", mnist.targets.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7751ff16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory used by training set: 35.888671875 MB\n",
      "Memory used by validation set: 4.486083984375 MB\n"
     ]
    }
   ],
   "source": [
    "# Show memory used by the dataset\n",
    "# Show memory used by the dataset\n",
    "print(\"Memory used by training set:\", mnist.data.element_size() * mnist.data[mnist_train.indices].nelement() / (1024 ** 2), \"MB\")\n",
    "print(\"Memory used by validation set:\", mnist.data.element_size() * mnist.data[mnist_val.indices].nelement() / (1024 ** 2), \"MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d75d26d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualise a random image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "image, label = random.choice(mnist)\n",
    "plt.imshow(image.squeeze(), cmap='gray')\n",
    "plt.title(f'Label: {label}')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4567800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Could use lightning here but we're keeping it simple so pure PyTorch.\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimplestNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimplestNN, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28 * 28, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08db9880",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.optim as optim\n",
    "model = SimplestNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2752b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation batch shape: torch.Size([6000, 28, 28])\n",
      "Validation targets shape: torch.Size([6000])\n",
      "Validation Loss before training : 2.318248987197876\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Validation batch shape:\", val_data.shape)\n",
    "print(\"Validation targets shape:\", val_targets.shape)\n",
    "\n",
    "\n",
    "val_outputs = model(val_data) \n",
    "val_loss = criterion(val_outputs, val_targets)\n",
    "print(\"Validation Loss before training :\", val_loss.item()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1e7b5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 validation Loss: 2.3182\n",
      "Epoch 2/100 validation Loss: 2.2584\n",
      "Epoch 3/100 validation Loss: 2.2015\n",
      "Epoch 4/100 validation Loss: 2.1434\n",
      "Epoch 5/100 validation Loss: 2.0819\n",
      "Epoch 6/100 validation Loss: 2.0170\n",
      "Epoch 7/100 validation Loss: 1.9493\n",
      "Epoch 8/100 validation Loss: 1.8802\n",
      "Epoch 9/100 validation Loss: 1.8104\n",
      "Epoch 10/100 validation Loss: 1.7401\n",
      "Epoch 11/100 validation Loss: 1.6699\n",
      "Epoch 12/100 validation Loss: 1.6000\n",
      "Epoch 13/100 validation Loss: 1.5310\n",
      "Epoch 14/100 validation Loss: 1.4633\n",
      "Epoch 15/100 validation Loss: 1.3977\n",
      "Epoch 16/100 validation Loss: 1.3344\n",
      "Epoch 17/100 validation Loss: 1.2737\n",
      "Epoch 18/100 validation Loss: 1.2155\n",
      "Epoch 19/100 validation Loss: 1.1598\n",
      "Epoch 20/100 validation Loss: 1.1068\n",
      "Epoch 21/100 validation Loss: 1.0565\n",
      "Epoch 22/100 validation Loss: 1.0090\n",
      "Epoch 23/100 validation Loss: 0.9644\n",
      "Epoch 24/100 validation Loss: 0.9228\n",
      "Epoch 25/100 validation Loss: 0.8839\n",
      "Epoch 26/100 validation Loss: 0.8478\n",
      "Epoch 27/100 validation Loss: 0.8142\n",
      "Epoch 28/100 validation Loss: 0.7830\n",
      "Epoch 29/100 validation Loss: 0.7539\n",
      "Epoch 30/100 validation Loss: 0.7269\n",
      "Epoch 31/100 validation Loss: 0.7016\n",
      "Epoch 32/100 validation Loss: 0.6782\n",
      "Epoch 33/100 validation Loss: 0.6563\n",
      "Epoch 34/100 validation Loss: 0.6360\n",
      "Epoch 35/100 validation Loss: 0.6171\n",
      "Epoch 36/100 validation Loss: 0.5995\n",
      "Epoch 37/100 validation Loss: 0.5832\n",
      "Epoch 38/100 validation Loss: 0.5679\n",
      "Epoch 39/100 validation Loss: 0.5537\n",
      "Epoch 40/100 validation Loss: 0.5404\n",
      "Epoch 41/100 validation Loss: 0.5278\n",
      "Epoch 42/100 validation Loss: 0.5161\n",
      "Epoch 43/100 validation Loss: 0.5050\n",
      "Epoch 44/100 validation Loss: 0.4946\n",
      "Epoch 45/100 validation Loss: 0.4848\n",
      "Epoch 46/100 validation Loss: 0.4755\n",
      "Epoch 47/100 validation Loss: 0.4668\n",
      "Epoch 48/100 validation Loss: 0.4586\n",
      "Epoch 49/100 validation Loss: 0.4508\n",
      "Epoch 50/100 validation Loss: 0.4434\n",
      "Epoch 51/100 validation Loss: 0.4364\n",
      "Epoch 52/100 validation Loss: 0.4298\n",
      "Epoch 53/100 validation Loss: 0.4235\n",
      "Epoch 54/100 validation Loss: 0.4175\n",
      "Epoch 55/100 validation Loss: 0.4118\n",
      "Epoch 56/100 validation Loss: 0.4064\n",
      "Epoch 57/100 validation Loss: 0.4012\n",
      "Epoch 58/100 validation Loss: 0.3963\n",
      "Epoch 59/100 validation Loss: 0.3915\n",
      "Epoch 60/100 validation Loss: 0.3870\n",
      "Epoch 61/100 validation Loss: 0.3827\n",
      "Epoch 62/100 validation Loss: 0.3786\n",
      "Epoch 63/100 validation Loss: 0.3747\n",
      "Epoch 64/100 validation Loss: 0.3709\n",
      "Epoch 65/100 validation Loss: 0.3672\n",
      "Epoch 66/100 validation Loss: 0.3637\n",
      "Epoch 67/100 validation Loss: 0.3604\n",
      "Epoch 68/100 validation Loss: 0.3571\n",
      "Epoch 69/100 validation Loss: 0.3540\n",
      "Epoch 70/100 validation Loss: 0.3510\n",
      "Epoch 71/100 validation Loss: 0.3481\n",
      "Epoch 72/100 validation Loss: 0.3453\n",
      "Epoch 73/100 validation Loss: 0.3426\n",
      "Epoch 74/100 validation Loss: 0.3400\n",
      "Epoch 75/100 validation Loss: 0.3374\n",
      "Epoch 76/100 validation Loss: 0.3349\n",
      "Epoch 77/100 validation Loss: 0.3325\n",
      "Epoch 78/100 validation Loss: 0.3302\n",
      "Epoch 79/100 validation Loss: 0.3279\n",
      "Epoch 80/100 validation Loss: 0.3258\n",
      "Epoch 81/100 validation Loss: 0.3236\n",
      "Epoch 82/100 validation Loss: 0.3216\n",
      "Epoch 83/100 validation Loss: 0.3195\n",
      "Epoch 84/100 validation Loss: 0.3175\n",
      "Epoch 85/100 validation Loss: 0.3156\n",
      "Epoch 86/100 validation Loss: 0.3137\n",
      "Epoch 87/100 validation Loss: 0.3119\n",
      "Epoch 88/100 validation Loss: 0.3101\n",
      "Epoch 89/100 validation Loss: 0.3083\n",
      "Epoch 90/100 validation Loss: 0.3066\n",
      "Epoch 91/100 validation Loss: 0.3050\n",
      "Epoch 92/100 validation Loss: 0.3033\n",
      "Epoch 93/100 validation Loss: 0.3017\n",
      "Epoch 94/100 validation Loss: 0.3001\n",
      "Epoch 95/100 validation Loss: 0.2985\n",
      "Epoch 96/100 validation Loss: 0.2970\n",
      "Epoch 97/100 validation Loss: 0.2955\n",
      "Epoch 98/100 validation Loss: 0.2940\n",
      "Epoch 99/100 validation Loss: 0.2926\n",
      "Epoch 100/100 validation Loss: 0.2912\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "for epoch in range(epochs): \n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    #Validate the model\n",
    "    with torch.no_grad():\n",
    "        #Loop through the validation set\n",
    "        val_loss = 0\n",
    "        val_outputs = model(val_data) \n",
    "        val_loss = criterion(val_outputs, val_targets)\n",
    "        print(f'Epoch {epoch+1}/{epochs} validation Loss: {val_loss.item():.4f}')\n",
    "\n",
    "    model.train() # Set the model to training mode\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    permuted_indices = torch.randperm(train_data.size(0))  # Shuffle the data\n",
    "    data = train_data[permuted_indices]\n",
    "    target = train_targets[permuted_indices]\n",
    "    batch_size=4096\n",
    "    for i in range(0, data.size(0), batch_size):\n",
    "\n",
    "        batch_data = data[i:i+batch_size].to(device)\n",
    "        batch_target = target[i:i+batch_size].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(batch_data)\n",
    "        loss = criterion(outputs, batch_target)\n",
    "        #print(f'Epoch {epoch}/{epochs} batch {i/128} training Loss: {loss.item():.4f}')\n",
    "        loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cee3c64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2898012697696686\n"
     ]
    }
   ],
   "source": [
    "val_outputs = model(val_data)\n",
    "val_loss = criterion(val_outputs, val_targets)\n",
    "print(\"Validation Loss:\", val_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aacf2977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect predictions in the test set: 501 out of 6000\n"
     ]
    }
   ],
   "source": [
    "test_outputs = model(test_data.to(device))\n",
    "test_targets = test_targets.to(device)\n",
    "# Print how many in the test set are classified incorrectly\n",
    "correct_predictions = (torch.argmax(test_outputs, dim=1) == test_targets).sum().item()\n",
    "total_predictions = test_targets.size(0)\n",
    "incorrect_predictions = total_predictions - correct_predictions\n",
    "print(f'Incorrect predictions in the test set: {incorrect_predictions} out of {total_predictions}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3e7a664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show the first incorrect prediction as an image\n",
    "test_images = test_data.cpu()\n",
    "incorrect_indices = (torch.argmax(test_outputs,dim=1) != test_targets).nonzero(as_tuple=True)[0]\n",
    "if incorrect_indices.numel() > 0:\n",
    "    first_incorrect_index = incorrect_indices[0]\n",
    "    incorrect_image = test_images[first_incorrect_index]\n",
    "    incorrect_label = test_targets[first_incorrect_index].item()\n",
    "    predicted_label = torch.argmax(test_outputs,dim=1)[first_incorrect_index].item()\n",
    "\n",
    "    plt.imshow(incorrect_image, cmap='gray')\n",
    "    plt.title(f'Incorrect Label: {incorrect_label}, Predicted: {predicted_label}')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
